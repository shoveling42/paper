# paper
조금이라도 읽어본 논문 리스트입니다. Keywords는 자의적인 기준에 의해 분류된 것이니 참고만 하시길 바랍니다.

This repo is about the list of papers which I've read a little bit. Keywords are for your information, i.e. they are classified by arbitary criteria.

| Tasks | Paper | Link | Keywords |
|:---------------|:-------------:|:-------------:|-------------:|
| NLP | What Changes Can Large-scale Language Models Bring? |[뉴스(News)](https://zdnet.co.kr/view/?no=20220128154803), [Service](https://www.ncloud.com/product/aiService/clovaStudio), [Paper](https://arxiv.org/pdf/2109.04650.pdf)|HyperCLOVA, LLM, No/Low Code AI|
| NLP | Attention Is All You Need |[Implementation](https://nlp.seas.harvard.edu/2018/04/03/attention.html), [Paper](https://arxiv.org/pdf/1706.03762.pdf)|Transformer, Self-Attention, Encoder, Decoder|
| NLP | Fine-tune BERT for Extractive Summarization |[Paper](https://arxiv.org/pdf/1903.10318.pdf)|BertSum, Extractive Summarization|
| NLP | A Survey on NLP based Text Summarization for Summarizing Product Reviews |[Paper](https://ieeexplore.ieee.org/document/9183355/keywords#keywords)|Extractive & Abstractive Summarization|
| NLP | Evaluating Large Language Models Trained on Code |[Paper](https://arxiv.org/pdf/2107.03374.pdf)|Machine Learning, Evaluation Code Correctness|
| NLP | Can Large Language Models Truly Understand Prompts? A Case Study with Negated Prompts |[Paper](https://arxiv.org/pdf/2209.12711.pdf)|TBD|
| NLP | Language Is Not All You Need: Aligning Perception with Language Models |[Paper](https://arxiv.org/pdf/2302.14045v1.pdf)|MLLM, TBD|
| NLP | Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer |[Paper](https://arxiv.org/pdf/1910.10683.pdf)|T5, TBD|
